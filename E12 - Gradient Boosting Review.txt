Presentando los métodos de clasificación existentes tenemos los métodos de Boosting que pretenden realizar una clasificación o una predicción teniendo en cuenta la disminución de los errores para lo cual estos algoritmos aprenden con la premisa de ajustar la predicción con base al error estimado de la regresión. Dentro de este encontramos dos algoritmos que parten del principio de optimización para generar esta aproximación los cuales son el Gradient Bossting y el XGBoosting.
Aunque similares en su ejecución ambos tienen una diferencia que hace que uno en comparación del otro sea mucho mas efectivo a la hora de realizar predicciones o clasificaciones.
Por un lado, el Gradient Boosting es un método iterativo lo cual se realizan arboles de manera aleatoria y en teoría “pequeños” los cuales de manera individual son estimadores pobres, pero que al analizarlos de manera conjunta generan una muy buena aproximación al modelo. Los errores resultantes del proceso son re evaluados a fin de disminuir estos de tal manera que el modelo de resultante se ajusta de manera adecuada a los datos a evaluar, dentro de este modelo es importante resaltar que los factores determinantes en la precisión del estimador son el numero de iteraciones y la profundidad de los árboles a evaluar 
Por otro lado, XGBoosting (Xtreme Gradient Boosting) El algoritmo realiza la clasificación de profundidad y repeticiones teniendo en cuenta parámetros asociados a la cantidad de datos que tiene para analizar el modelo. Por un lado, la profundidad de los arboles es definida por con menos variables lo cual hace que el modelo se retraiga de manera más exigente, por otro lado para llegar al optimo el algoritmo realiza una aproximación por Newton-Raphson lo cual hace que se llegue a una solución de meneara mucho más rápida.
